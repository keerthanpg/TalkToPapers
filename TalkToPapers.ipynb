{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/data-hound/talktopapers/blob/master/TalkToPapers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This colab lets you upload a paper to your drive and talk to it using Open AI's embeddings. \n",
        "\n"
      ],
      "metadata": {
        "id": "xLWo3YPkZXbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Dependencies\n",
        "!pip install pypdf\n",
        "!pip install wget\n",
        "!pip install PyPDF2\n",
        "!pip install tiktoken\n",
        "!pip install openai"
      ],
      "metadata": {
        "id": "fiiqxuQRgPbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-pMQIv3jf4eU"
      },
      "outputs": [],
      "source": [
        "#@title Import Dependencies\n",
        "import sys\n",
        "from collections import defaultdict\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import patches\n",
        "import argparse\n",
        "from pypdf import PdfReader\n",
        "from pathlib import Path\n",
        "import requests\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import openai \n",
        "import tiktoken\n",
        "from openai.embeddings_utils import get_embedding, cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "Specify api key, mount Gdrive"
      ],
      "metadata": {
        "id": "YixvdHYjXOQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setup\n",
        "drive.mount('/content/drive')\n",
        "openai.api_key = 'sk-somerandomcombination0falpanumerics' #@param {type:\"string\"}\n",
        "sys.path.append(\"../\")"
      ],
      "metadata": {
        "id": "qlbOJyU0yIfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #Download Paper\n",
        "\n",
        "#@markdown \n",
        "\n",
        "#@markdown #####**Paper URL:**\n",
        "\n",
        "# ! wget https://export.arxiv.org/pdf/2104.08894.pdf \n",
        "\n",
        "url = 'http://export.arxiv.org/pdf/1706.03762.pdf'#@param {type:\"string\"}\n",
        "verbose = True #@param {type:\"boolean\"}\n",
        "\n",
        "import subprocess\n",
        "\n",
        "def runcmd(cmd, verbose = False, *args, **kwargs):\n",
        "\n",
        "    process = subprocess.Popen(\n",
        "        cmd,\n",
        "        stdout = subprocess.PIPE,\n",
        "        stderr = subprocess.PIPE,\n",
        "        text = True,\n",
        "        shell = True\n",
        "    )\n",
        "    std_out, std_err = process.communicate()\n",
        "    if verbose:\n",
        "        print(std_out.strip(), std_err)\n",
        "    pass\n",
        "\n",
        "filename = '/content/drive/MyDrive/' + 'NeurIPS-2020-deep-graph-pose-a-semi-supervised-deep-graphical-model-for-improved-animal-pose-tracking-Paper.pdf'\n",
        "\n",
        "if 'MyDrive' not in url:\n",
        "  runcmd(\"wget {}\".format(url), verbose=verbose)\n",
        "\n",
        "  filename = '/content/' + url.split('/')[-1]\n",
        "else:\n",
        "  filename = url\n",
        "\n",
        "import os\n",
        "print(\"Checking filename exists...\")\n",
        "assert os.path.exists(filename), \"File does not exist in the given location\"\n",
        "print(\"File Exists\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0KDpvUPxb9Y",
        "outputId": "aa138a07-06e5-4503-bbb8-5e4c0c8e564d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " --2023-01-16 05:14:13--  http://export.arxiv.org/pdf/1706.03762.pdf\n",
            "Resolving export.arxiv.org (export.arxiv.org)... 128.84.21.203\n",
            "Connecting to export.arxiv.org (export.arxiv.org)|128.84.21.203|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2201700 (2.1M) [application/pdf]\n",
            "Saving to: ‘1706.03762.pdf’\n",
            "\n",
            "     0K .......... .......... .......... .......... ..........  2% 76.0M 0s\n",
            "    50K .......... .......... .......... .......... ..........  4% 1.36M 1s\n",
            "   100K .......... .......... .......... .......... ..........  6% 1.41M 1s\n",
            "   150K .......... .......... .......... .......... ..........  9% 65.5M 1s\n",
            "   200K .......... .......... .......... .......... .......... 11%  215M 1s\n",
            "   250K .......... .......... .......... .......... .......... 13% 1.38M 1s\n",
            "   300K .......... .......... .......... .......... .......... 16% 80.5M 1s\n",
            "   350K .......... .......... .......... .......... .......... 18% 1.43M 1s\n",
            "   400K .......... .......... .......... .......... .......... 20% 47.2M 1s\n",
            "   450K .......... .......... .......... .......... .......... 23% 93.1M 0s\n",
            "   500K .......... .......... .......... .......... .......... 25% 1.41M 1s\n",
            "   550K .......... .......... .......... .......... .......... 27% 63.3M 0s\n",
            "   600K .......... .......... .......... .......... .......... 30% 1.42M 0s\n",
            "   650K .......... .......... .......... .......... .......... 32% 70.0M 0s\n",
            "   700K .......... .......... .......... .......... .......... 34%  110M 0s\n",
            "   750K .......... .......... .......... .......... .......... 37% 1.41M 0s\n",
            "   800K .......... .......... .......... .......... .......... 39% 51.6M 0s\n",
            "   850K .......... .......... .......... .......... .......... 41% 1.42M 0s\n",
            "   900K .......... .......... .......... .......... .......... 44% 69.4M 0s\n",
            "   950K .......... .......... .......... .......... .......... 46%  221M 0s\n",
            "  1000K .......... .......... .......... .......... .......... 48% 1.39M 0s\n",
            "  1050K .......... .......... .......... .......... .......... 51% 61.2M 0s\n",
            "  1100K .......... .......... .......... .......... .......... 53% 1.43M 0s\n",
            "  1150K .......... .......... .......... .......... .......... 55% 33.7M 0s\n",
            "  1200K .......... .......... .......... .......... .......... 58%  127M 0s\n",
            "  1250K .......... .......... .......... .......... .......... 60% 1.41M 0s\n",
            "  1300K .......... .......... .......... .......... .......... 62% 88.7M 0s\n",
            "  1350K .......... .......... .......... .......... .......... 65%  249M 0s\n",
            "  1400K .......... .......... .......... .......... .......... 67% 1.38M 0s\n",
            "  1450K .......... .......... .......... .......... .......... 69%  125M 0s\n",
            "  1500K .......... .......... .......... .......... .......... 72% 1.41M 0s\n",
            "  1550K .......... .......... .......... .......... .......... 74% 44.4M 0s\n",
            "  1600K .......... .......... .......... .......... .......... 76%  147M 0s\n",
            "  1650K .......... .......... .......... .......... .......... 79% 1.40M 0s\n",
            "  1700K .......... .......... .......... .......... .......... 81%  149M 0s\n",
            "  1750K .......... .......... .......... .......... .......... 83% 1.40M 0s\n",
            "  1800K .......... .......... .......... .......... .......... 86% 67.8M 0s\n",
            "  1850K .......... .......... .......... .......... .......... 88%  147M 0s\n",
            "  1900K .......... .......... .......... .......... .......... 90% 1.40M 0s\n",
            "  1950K .......... .......... .......... .......... .......... 93%  137M 0s\n",
            "  2000K .......... .......... .......... .......... .......... 95% 1.42M 0s\n",
            "  2050K .......... .......... .......... .......... .......... 97% 43.3M 0s\n",
            "  2100K .......... .......... .......... .......... .......... 99%  119M 0s\n",
            "  2150K                                                       100%  186G=0.6s\n",
            "\n",
            "2023-01-16 05:14:14 (3.46 MB/s) - ‘1706.03762.pdf’ saved [2201700/2201700]\n",
            "\n",
            "\n",
            "Checking filename exists...\n",
            "File Exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parse PDF to text"
      ],
      "metadata": {
        "id": "KhCZZLrAYYP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_paper(path):\n",
        "  print(\"Parsing paper\")\n",
        "  reader = PdfReader(path)\n",
        "  number_of_pages = len(reader.pages)\n",
        "  print(f\"Total number of pages: {number_of_pages}\")\n",
        "  paper_text = []\n",
        "  for i in range(number_of_pages):\n",
        "    page = reader.pages[i]\n",
        "    page_text = []\n",
        "\n",
        "    def visitor_body(text, cm, tm, fontDict, fontSize):\n",
        "      x = tm[4]\n",
        "      y = tm[5]\n",
        "      # ignore header/footer\n",
        "      if (y > 50 and y < 720) and (len(text.strip()) > 1):\n",
        "        page_text.append({\n",
        "          'fontsize': fontSize,\n",
        "          'text': text.strip().replace('\\x03', ''),\n",
        "          'x': x,\n",
        "          'y': y\n",
        "        })\n",
        "\n",
        "    _ = page.extract_text(visitor_text=visitor_body)\n",
        "\n",
        "    blob_font_size = None\n",
        "    blob_text = ''\n",
        "    processed_text = []\n",
        "\n",
        "    for t in page_text:\n",
        "      if t['fontsize'] == blob_font_size:\n",
        "        blob_text += f\" {t['text']}\"\n",
        "      else:\n",
        "        if blob_font_size is not None and len(blob_text) > 1:\n",
        "          processed_text.append({\n",
        "            'fontsize': blob_font_size,\n",
        "            'text': blob_text,\n",
        "            'page': i\n",
        "          })\n",
        "        blob_font_size = t['fontsize']\n",
        "        blob_text = t['text']\n",
        "    paper_text += processed_text\n",
        "  return paper_text"
      ],
      "metadata": {
        "id": "DhL684zgLDF-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paper_text = parse_paper(filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qm9GI3TLfeD",
        "outputId": "838990d4-40ce-417e-ac9f-12a7f8e5591b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing paper\n",
            "Total number of pages: 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert not (paper_text == []), \"Paper Text should be non-empty\""
      ],
      "metadata": {
        "id": "Q8csM7kL0Wtc"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply a small filter"
      ],
      "metadata": {
        "id": "a4DhysCqYcup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_paper_text = []\n",
        "for row in paper_text:\n",
        "  if len(row['text']) < 30:\n",
        "    continue\n",
        "  filtered_paper_text.append(row)"
      ],
      "metadata": {
        "id": "orUj9AgyLvMy"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert to dataframe and inspect"
      ],
      "metadata": {
        "id": "LGc-CIqdYgF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(filtered_paper_text)\n",
        "print(df.shape)\n",
        "df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "3ezUH8kygHDL",
        "outputId": "f5977ea7-3949-42ae-b7e6-97721c59249a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(43, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   fontsize                                               text  page\n",
              "0    9.9626  Ashish Vaswani Google Brain avaswani@google.co...     0\n",
              "1    9.9626  University of Toronto aidan@cs.toronto.edu Łuk...     0\n",
              "2    9.9626  The dominant sequence transduction models are ...     0\n",
              "3    9.9626  Recurrent neural networks, long short-term mem...     0\n",
              "4    8.9664  Equal contribution. Listing order is random. J...     0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8e4ab1bb-d7b5-46b3-96bf-eb64d0fc3471\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fontsize</th>\n",
              "      <th>text</th>\n",
              "      <th>page</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9.9626</td>\n",
              "      <td>Ashish Vaswani Google Brain avaswani@google.co...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9.9626</td>\n",
              "      <td>University of Toronto aidan@cs.toronto.edu Łuk...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9.9626</td>\n",
              "      <td>The dominant sequence transduction models are ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9.9626</td>\n",
              "      <td>Recurrent neural networks, long short-term mem...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8.9664</td>\n",
              "      <td>Equal contribution. Listing order is random. J...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8e4ab1bb-d7b5-46b3-96bf-eb64d0fc3471')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8e4ab1bb-d7b5-46b3-96bf-eb64d0fc3471 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8e4ab1bb-d7b5-46b3-96bf-eb64d0fc3471');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate pdf embeddings"
      ],
      "metadata": {
        "id": "zriCg1dxYkE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = \"text-embedding-ada-002\"\n",
        "embeddings = df.text.apply([lambda x: get_embedding(x, engine=embedding_model)])\n",
        "df[\"embeddings\"] = embeddings"
      ],
      "metadata": {
        "id": "JSgBs0QVApeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GBQjg5NGFf4",
        "outputId": "e2bbb97c-b529-4357-928d-39a18ecd1375"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(36, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embed query and Search\n",
        "\n",
        "We return the chunk in pdf with highest cosine similarity with query embedding"
      ],
      "metadata": {
        "id": "_Ga0r7-uY10Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_reviews(df, query, n=3, pprint=True):\n",
        "    query_embedding = get_embedding(\n",
        "        query,\n",
        "        engine=\"text-embedding-ada-002\"\n",
        "    )\n",
        "    df[\"similarity\"] = df.embeddings.apply(lambda x: cosine_similarity(x, query_embedding))\n",
        "\n",
        "    results = (\n",
        "        df.sort_values(\"similarity\", ascending=False)\n",
        "        \n",
        "    )\n",
        "    return results"
      ],
      "metadata": {
        "id": "QZZs5pT9-xtl"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Few Example Results"
      ],
      "metadata": {
        "id": "jTeveJeQZImC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "T4-HMlmuZQX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Query!\n",
        "\n",
        "#@markdown \n",
        "\n",
        "#@markdown #####**Query:**\n",
        "query = \"What embeddings were used\" #@param {type:\"string\"}\n",
        "n=3 #@param {type:'integer'}\n",
        "\n",
        "results = search_reviews(df, query=query, n=3)\n",
        "results.iloc[0]['text']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "cellView": "form",
        "id": "_K7ss1tU5dpc",
        "outputId": "e5771c16-74cc-49c4-8bc4-372f7283388b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'We also experimented with using learned positional embeddings [ 8] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = search_reviews(df, \"explain how multi head self attention works\", n=3)\n",
        "results.iloc[0]['text']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "9VdQYHZ8_vqd",
        "outputId": "ba537b92-5916-4c64-9d07-ebe392982be8"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [20], ByteNet [ 15] and ConvS2S [ 8], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, 19]. End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [28]. To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [14, 15] and [8].'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = search_reviews(df, \"explain the training procedure\", n=3)\n",
        "results.iloc[0]['text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "v63tyA1sWwxL",
        "outputId": "9a897eb8-f323-48dd-bf15-1243d899dd02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This section describes the training regime for our models. 5.1 Training Data and Batching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source- target vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens. 5.2 Hardware and Schedule We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days). 5.3 Optimizer We used the Adam optimizer [ 20] with = 0 = 0 98 and = 10 . We varied the learning rate over the course of training, according to the formula: lrate'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    }
  ]
}